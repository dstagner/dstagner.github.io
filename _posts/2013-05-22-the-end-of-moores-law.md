---
layout: post
title: The End of Moore's Law, sort of
---

Moore’s Law has been in force my entire life. When I first touched computers back in the early 1980s, 16k was typical memory and 300k was a huge floppy disk. Now, I have 8gb of RAM and 750gb of disk sitting in my lap – a million times the memory of those old TRS-80s, more raw computing power than even the greatest supercomputers of my childhood. CPU power and most other measures have grown equally.

So why do I say it’s the end? Let’s look at the new mammal currently out-evolving the desktop dinosaurs… smart phones. My phone (currently an iPhone 4S with 64gb) is an amazing device. But it has certain limitations. The limits aren’t from the computer parts, which will continue to improve with Moore’s Law for a while, but rather with the non-computer parts.

First, its screen size is inherently limited, to just a few inches across. It can’t be significantly larger than it is now, not if it’s going to fit easily in a pocket or a hand. Various Android-based phones try to be slightly bigger, but at a cost of awkwardness. The newer iPhone 5 is slightly longer, but not wider or thicker. And the Retina screens on the latest iPhones approach the resolution limits of human eyesight.  In a couple more years, everyone will have screens that good. Screens much better than an iPhone 5 are just bouncing the rubble.

Second, its battery size is limited, by the same physical boundaries. And unlike computers, batteries are not beneficiaries of Moore’s Law. They’ve improved substantially, but not the exponential increase we get from silicon. One of the common flaws in thinking about tech is that everything can improve the way silicon improves. Not true. Batteries will flatten out soon.

Which leads us to another problem – screens won’t consume less energy, and neither will antennas. Computer screens, lit from the inside, need to emit a certain number of lumens of light to be clearly visible. Modern LED screens are already highly efficient. I haven’t found hard numbers offhand, but I’ll bet they’re already over 50%. And the antenna of a cell phone is, at heart, an analog radio. It requires a certain amount of power to both receive and send signals, especially send them. This has been optimized for a century. There simply aren’t any order-of-magnitude improvements to be had in energy efficiency for the hungry screen and antenna, and there aren’t any for the tiny battery. We might get, what, twice the life that we currently get? Even ten times? That’s peanuts compared to Moore’s Law.

Third, as mentioned earlier, is the fact that a cell phone is basically a radio. They operate on certain frequencies, shared with dozens (or even thousands) of devices in the same area. All the clever software tricks in the world won’t get around the fact that only so many bits of information can be communicated in a given frequency range, and we’re getting to limits there as well. Again, we might double our bandwidth, or even squeeze out another order of magnitude, but that’s it.

So our phones, the most important computers in our lives, are inherently limited in terms of information density onscreen, lifespan without recharging, and communication range and bandwidth. Moore’s Law won’t save us from these limits. Barring some previously-unimagined breakthrough, I can’t see phones a century from now being radically better than they are today.

Of course, I could very well be wrong.
