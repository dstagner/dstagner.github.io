---
layout: post
title: "The Fine Line"
date: 2014-09-09 00:54:50 -0500
comments: true
categories: [software, philosophy]
published: false
---
<img src="http://25.media.tumblr.com/tumblr_lleytqlZ5m1qchzcgo1_500.png" align="center"/>

Almost all software hovers around a line between "barely works" and "doesn't work". Software that really works well, that is reliable and efficient and complete, is very rare. 

Why? WTF? Do we suck so bad? Do our tools suck so bad? 

The answer is neither. Programmers are terrific, and our tools are fantastic and get better all the time. No, the cause lies elsewhere...

<!-- more -->

If a project of any consequence is really driven as hard as it can be driven, it will *always barely work*. If you're not rubbing up against the "barely works" line, it means you're putting effort into the project for quality, not functionality. This isn't the same as ignoring the project. A neglected project barely works (or doesn't work). 

<img src="http://aws.hackingchristianity.net/wp-content/files/here-there-be-dragons.jpg" align="center"/>

## Here there be dragons
The line between "barely works" and "doesn't work" isn't a divider. It's more like a border - a border to the edge of the world. Here there be dragons! The reason for this is straightforward... once software stops working, it's hard to make it *more* not work. Instead, effort is focused on making it work again, or it gets neglected or abandoned. Even continuing to make it worse is difficult, as the software itself prevents developers from making "progress" into failure. 
